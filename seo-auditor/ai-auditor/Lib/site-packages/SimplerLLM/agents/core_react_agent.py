from pydantic import BaseModel
from SimplerLLM.language.llm import LLM
from SimplerLLM.prompts.messages_template import MessagesTemplate
from SimplerLLM.prompts.hub.agentic_prompts import react_core_agent_system_prompt
from SimplerLLM.utils.custom_verbose import verbose_print
from SimplerLLM.tools.json_helpers import extract_json_from_text
from SimplerLLM.tools.predefined_tools import PREDEFINED_TOOLS
import json

class ReActAgentResponse(BaseModel):
    """
    A model for storing the response from the ReActAgent.
    Attributes:
        user_query (str): The original query from the user.
        agent_output (str): The output generated by the tool or directly from the language model.
    """
    user_query: str
    agent_output: str

class ReActAgent:
    """
    A tool managing agent that interfaces with a language model to process and respond to user queries.

    Attributes:
        verbose (bool): If set to True, enables detailed logging for debugging purposes.
        llm_instance (LLM): An instance of a language model from the SimplerLLM library.
        available_actions (dict): A dictionary to store available tool functions mapped to their descriptions.
        system_prompt_template (str): A template used to construct the system prompt for the language model.
    """
    
    def __init__(self, llm_instance: LLM, agent_system_prompt=react_core_agent_system_prompt, verbose: bool=False):
        """
        Initializes the ReActAgent with necessary parameters.

        Parameters:
            llm_instance (LLM): An instance of the language model to be used.
            agent_system_prompt (str): The system prompt template for generating responses.
            verbose (bool): Flag to turn on verbose logging for debugging.
        """
        self.verbose = verbose
        self.llm_instance = llm_instance
        self.available_actions = {}
        self.system_prompt_template = agent_system_prompt
        if self.verbose:
            verbose_print("Initialized ReActAgent with verbose mode.", "info")

    def add_tool(self, tool_function):
        """
        Adds a tool to the agent's available actions.

        Parameters:
            tool_function (callable): The function representing the tool to be added.

        Raises:
            ValueError: If the tool function is not predefined or properly decorated as a custom tool.
        """
        try:
            if tool_function in PREDEFINED_TOOLS.values():
                tool_name = next(name for name, func in PREDEFINED_TOOLS.items() if func == tool_function)
                description = tool_function.__doc__.strip()
            elif hasattr(tool_function, 'is_custom_tool') and tool_function.is_custom_tool:
                tool_name = tool_function.__name__
                description = tool_function.description
            else:
                raise ValueError("Tool function must be predefined or decorated as a custom tool.")

            self.available_actions[tool_name] = {
                "function": tool_function,
                "description": description
            }
            if self.verbose:
                verbose_print(f"Added tool: {tool_name} with description: {description}", "info")
        except Exception as e:
            if self.verbose:
                verbose_print(f"Error in add_tool: {str(e)}", "error")
            raise

    def construct_system_prompt(self):
        """
        Constructs the system prompt from the available actions to guide the language model.

        Returns:
            str: The formatted system prompt incorporating descriptions of all available actions.
        """
        actions_description = "\n".join(
            [f"{name}:\n {details['description']}"
             for name, details in self.available_actions.items()]
        )
        return self.system_prompt_template.format(actions_list=actions_description)

    def generate_response(self, user_query: str = None, messages: MessagesTemplate = None, max_turns = 5 ):
        """
        Generates a response to a user query using the language model and available tools.

        Parameters:
            user_query (str): The user's query to process.
            execute_tool (bool): Whether to execute the tool function if applicable.
            message_history: A history of past messages (not currently used).

        Returns:
            ReActAgentResponse: The response from the agent, either as direct LLM output or processed by a tool.
        """
        agent_system_prompt = self.construct_system_prompt()

        if user_query and messages:
            raise ValueError("Only one of 'user_query' or 'messages' should be provided.")
 

        agent_messages = MessagesTemplate()
        

        turn_count = 1
        user_request = ""

        if user_query:
            agent_messages.add_user_message(user_query)
            user_request = user_query
        if messages:
            agent_messages.prepend_messages(messages.get_messages())
            user_request = agent_messages.get_messages()[-1]['content']

        
        tool_agent_response = ReActAgentResponse(
                        user_query=user_request,
                        agent_output=""
                    )


        while turn_count <= max_turns:
            if self.verbose:
                verbose_print(f"Turn: {turn_count}")
                verbose_print("----------------------")

            turn_count += 1
            #used to extract last user query in case messages were provided instead of single prompt
            history = agent_messages.get_messages()
            if user_query:
                
                agent_response = self.llm_instance.generate_response(messages=history, system_prompt=agent_system_prompt)
            if messages:

                agent_response = self.llm_instance.generate_response(messages=history, system_prompt=agent_system_prompt)


            agent_messages.add_assistant_message(agent_response)

            final_response = agent_response

            if self.verbose:
                verbose_print(f"LLM First Response: {agent_response}", "info")

            # Attempt to extract a JSON action from the LLM response
            try:
                action_json = extract_json_from_text(agent_response)
            except json.JSONDecodeError:
                if self.verbose:
                    verbose_print("Failed to decode JSON from response.", "error")
                action_json = None

            # Handle response based on the presence of an actionable JSON
            if action_json:
                if 'function_name' in action_json[0]:
                    function_name = action_json[0].get('function_name')
                    function_params = action_json[0].get('function_params', {})
                    if function_name not in self.available_actions:
                        raise Exception(f"Unknown action: {function_name}")

                    

                    
                    action_function = self.available_actions[function_name]["function"]
                    result = action_function(**function_params)

                    if self.verbose:
                        verbose_print(f"Executed {function_name} with parameters {function_params}.", "info")

                    observation = f"Observation: {result}"

                    
                    agent_messages.add_user_message(observation)

                    if self.verbose:
                        verbose_print("----------------------")

                

                else:
                    break
            else:
                if self.verbose:
                    verbose_print("No action JSON found; returning LLM response directly.", "info")
                break

                    #Check if the final_response is the final
        
        
        try:
            is_final_json = extract_json_from_text(agent_response)
            if is_final_json:
                if 'final_answer' in is_final_json[0]:
                    tool_agent_response.agent_output = is_final_json[0]['final_answer']
                    return tool_agent_response

        except json.JSONDecodeError:
                pass

        tool_agent_response.agent_output = final_response
        return tool_agent_response